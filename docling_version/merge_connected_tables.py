"""
í˜ì´ì§€ë¥¼ ë„˜ì–´ê°€ëŠ” í…Œì´ë¸”ì„ ì°¾ì•„ì„œ ë³‘í•©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
í…ìŠ¤íŠ¸ ì—°ê²°ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ê°™ì€ í…Œì´ë¸”ë¡œ íŒë‹¨
"""
import json
import os
from pathlib import Path
from typing import List, Dict, Tuple
import re
# from difflib import SequenceMatcher  # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (sentence-transformersë¡œ ëŒ€ì²´)
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from reportlab.lib.pagesizes import A4
from reportlab.lib import colors
from reportlab.lib.units import cm
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, PageBreak, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.enums import TA_LEFT, TA_CENTER

# íƒ€ì´í‹€ ë¶„ë¥˜ê¸°: ML ëª¨ë¸ ì‚¬ìš© (transformers í•„ìš”)
# fallbackìœ¼ë¡œ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œë„ í¬í•¨
try:
    from title_classifier_ml import select_best_title_with_model
    USE_ML_CLASSIFIER = True
except ImportError:
    print("ê²½ê³ : transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ê·œì¹™ ê¸°ë°˜ íƒ€ì´í‹€ ì„ íƒ ì‚¬ìš©")
    from title_classifier import select_best_title
    USE_ML_CLASSIFIER = False

# íƒ€ì´í‹€ ìœ ì‚¬ë„ ë¹„êµìš© sentence-transformers
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    _similarity_model = None

    def get_similarity_model():
        """ì „ì—­ sentence-transformer ëª¨ë¸ (lazy loading)"""
        global _similarity_model
        if _similarity_model is None:
            print("  ğŸ“Š íƒ€ì´í‹€ ìœ ì‚¬ë„ ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒ)", flush=True)
            _similarity_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # ë‹¤êµ­ì–´ ì§€ì›, 50MB
            print("  âœ… ìœ ì‚¬ë„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ", flush=True)
        return _similarity_model

    USE_SIMILARITY_MODEL = True
except ImportError:
    print("ê²½ê³ : sentence-transformers ì—†ìŒ, ê¸°ë³¸ ìœ ì‚¬ë„ ì‚¬ìš©")
    USE_SIMILARITY_MODEL = False


def register_korean_font():
    """í•œê¸€ í°íŠ¸ ë“±ë¡"""
    try:
        # Windows ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
        pdfmetrics.registerFont(TTFont('Korean', 'malgun.ttf'))
        return 'Korean'
    except:
        try:
            pdfmetrics.registerFont(TTFont('Korean', 'C:/Windows/Fonts/malgun.ttf'))
            return 'Korean'
        except:
            print("Warning: Could not load Korean font. Using default font.")
            return 'Helvetica'


def is_point_in_bbox(x: float, y: float, bbox: Dict) -> bool:
    """ì ì´ bbox ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸"""
    if not bbox:
        return False
    left = bbox.get('l', 0)
    right = bbox.get('r', 0)
    top = bbox.get('t', 0)
    bottom = bbox.get('b', 0)
    coord_origin = bbox.get('coord_origin', 'BOTTOMLEFT')

    if coord_origin == 'BOTTOMLEFT':
        # BOTTOMLEFT: yê°€ ìœ„ë¡œ ê°ˆìˆ˜ë¡ ì¦ê°€
        return left <= x <= right and bottom <= y <= top
    else:  # TOPLEFT
        # TOPLEFT: yê°€ ì•„ë˜ë¡œ ê°ˆìˆ˜ë¡ ì¦ê°€
        return left <= x <= right and top <= y <= bottom


def is_text_in_any_table(text_bbox: Dict, all_tables: List[Dict], exclude_table: Dict) -> bool:
    """í…ìŠ¤íŠ¸ê°€ ë‹¤ë¥¸ í…Œì´ë¸” ì•ˆì— ìˆëŠ”ì§€ í™•ì¸"""
    if not text_bbox:
        return False

    # í…ìŠ¤íŠ¸ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
    text_center_x = (text_bbox.get('l', 0) + text_bbox.get('r', 0)) / 2
    text_center_y = (text_bbox.get('t', 0) + text_bbox.get('b', 0)) / 2

    exclude_bbox = exclude_table.get('prov', [{}])[0].get('bbox', {})

    for table in all_tables:
        table_bbox = table.get('prov', [{}])[0].get('bbox', {})

        # í˜„ì¬ í…Œì´ë¸” ì œì™¸
        if table_bbox == exclude_bbox:
            continue

        # í…ìŠ¤íŠ¸ ì¤‘ì‹¬ì ì´ í…Œì´ë¸” ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸
        if is_point_in_bbox(text_center_x, text_center_y, table_bbox):
            return True

    return False


def detect_repeated_headers_footers(all_texts: List[Dict], min_pages: int = 3) -> set:
    """ë¬¸ì„œ ë‚´ì—ì„œ ë°˜ë³µë˜ëŠ” header/footer í…ìŠ¤íŠ¸ íŒ¨í„´ ê°ì§€"""
    # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ìœ„ì¹˜ ìˆ˜ì§‘ (ìƒë‹¨ 100px, í•˜ë‹¨ 100px)
    page_top_texts = {}  # {page_no: [(text, y_position)]}
    page_bottom_texts = {}

    for text_obj in all_texts:
        text_content = text_obj.get('text', '').strip()
        if not text_content or len(text_content) < 2:
            continue

        page_no = text_obj.get('prov', [{}])[0].get('page_no', -1)
        if page_no == -1:
            continue

        text_bbox = text_obj.get('prov', [{}])[0].get('bbox', {})
        if not text_bbox:
            continue

        coord_origin = text_bbox.get('coord_origin', 'BOTTOMLEFT')

        if coord_origin == 'BOTTOMLEFT':
            y_pos = text_bbox.get('t', 0)  # ìƒë‹¨ y ì¢Œí‘œ
            # ìƒë‹¨ 100px ë‚´ì˜ í…ìŠ¤íŠ¸ (y > 692 for 792 height)
            if y_pos > 692:
                if page_no not in page_top_texts:
                    page_top_texts[page_no] = []
                page_top_texts[page_no].append(text_content)
            # í•˜ë‹¨ 100px ë‚´ì˜ í…ìŠ¤íŠ¸ (y < 100)
            elif y_pos < 100:
                if page_no not in page_bottom_texts:
                    page_bottom_texts[page_no] = []
                page_bottom_texts[page_no].append(text_content)
        else:  # TOPLEFT
            y_pos = text_bbox.get('t', 0)
            # ìƒë‹¨ 100px ë‚´ì˜ í…ìŠ¤íŠ¸ (y < 100)
            if y_pos < 100:
                if page_no not in page_top_texts:
                    page_top_texts[page_no] = []
                page_top_texts[page_no].append(text_content)
            # í•˜ë‹¨ 100px ë‚´ì˜ í…ìŠ¤íŠ¸ (y > 692 for 792 height)
            elif y_pos > 692:
                if page_no not in page_bottom_texts:
                    page_bottom_texts[page_no] = []
                page_bottom_texts[page_no].append(text_content)

    # ë°˜ë³µë˜ëŠ” í…ìŠ¤íŠ¸ ì°¾ê¸°
    repeated_texts = set()

    # í…ìŠ¤íŠ¸ ë¹ˆë„ ê³„ì‚°
    from collections import Counter

    # ìƒë‹¨ í…ìŠ¤íŠ¸ ì¤‘ ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ë°˜ë³µë˜ëŠ” ê²ƒ
    all_top_texts = [text for texts in page_top_texts.values() for text in texts]
    top_counter = Counter(all_top_texts)
    for text, count in top_counter.items():
        if count >= min_pages:  # ìµœì†Œ Nê°œ í˜ì´ì§€ì—ì„œ ë°˜ë³µ
            repeated_texts.add(text)

    # í•˜ë‹¨ í…ìŠ¤íŠ¸ ì¤‘ ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ë°˜ë³µë˜ëŠ” ê²ƒ
    all_bottom_texts = [text for texts in page_bottom_texts.values() for text in texts]
    bottom_counter = Counter(all_bottom_texts)
    for text, count in bottom_counter.items():
        if count >= min_pages:
            repeated_texts.add(text)

    return repeated_texts


def find_table_title(table: Dict, all_texts: List[Dict], all_tables: List[Dict] = None, page_height: float = 792.0, repeated_patterns: set = None) -> str:
    """í…Œì´ë¸” ìœ„ 300px ë‚´ì˜ í…ìŠ¤íŠ¸ë¥¼ íƒ€ì´í‹€ë¡œ ì¶”ì¶œ (ë‹¤ë¥¸ í…Œì´ë¸” ì•ˆì˜ í…ìŠ¤íŠ¸ ì œì™¸)"""
    table_bbox = table.get('prov', [{}])[0].get('bbox', {})
    table_page = table.get('prov', [{}])[0].get('page_no', -1)

    if not table_bbox or table_page == -1:
        return ""

    # ì¢Œí‘œê³„ í™•ì¸
    coord_origin = table_bbox.get('coord_origin', 'BOTTOMLEFT')

    if coord_origin == 'BOTTOMLEFT':
        # BOTTOMLEFT: yê°€ ìœ„ë¡œ ê°ˆìˆ˜ë¡ ì¦ê°€
        # í…Œì´ë¸”ì˜ ìƒë‹¨ y ì¢Œí‘œ
        table_top_y = table_bbox.get('t', 0)
    else:  # TOPLEFT
        # TOPLEFT: yê°€ ì•„ë˜ë¡œ ê°ˆìˆ˜ë¡ ì¦ê°€
        # í…Œì´ë¸”ì˜ ìƒë‹¨ y ì¢Œí‘œ (ì‘ì€ ê°’)
        table_top_y = table_bbox.get('t', 0)

    # í…Œì´ë¸” ìœ„ 300px ì˜ì—­ì˜ í…ìŠ¤íŠ¸ ì°¾ê¸°
    title_candidates = []

    for text_obj in all_texts:
        text_page = text_obj.get('prov', [{}])[0].get('page_no', -1)

        # ê°™ì€ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë§Œ í™•ì¸
        if text_page != table_page:
            continue

        text_bbox = text_obj.get('prov', [{}])[0].get('bbox', {})
        if not text_bbox:
            continue

        text_coord_origin = text_bbox.get('coord_origin', 'BOTTOMLEFT')

        if coord_origin == 'BOTTOMLEFT' and text_coord_origin == 'BOTTOMLEFT':
            # BOTTOMLEFT: í…Œì´ë¸” ìœ„ì— ìˆëŠ” í…ìŠ¤íŠ¸ëŠ” yê°’ì´ ë” í¼
            text_bottom_y = text_bbox.get('b', 0)
            text_top_y = text_bbox.get('t', 0)

            # í…ìŠ¤íŠ¸ì˜ í•˜ë‹¨ì´ í…Œì´ë¸” ìƒë‹¨ë³´ë‹¤ ìœ„ì— ìˆëŠ”ì§€ í™•ì¸
            if text_bottom_y > table_top_y:
                distance = text_bottom_y - table_top_y
                if distance <= 300:
                    text_content = text_obj.get('text', '').strip()
                    if text_content and len(text_content) > 0:
                        title_candidates.append({
                            'text': text_content,
                            'distance': distance,
                            'y_position': text_bottom_y
                        })

        elif coord_origin == 'TOPLEFT' and text_coord_origin == 'TOPLEFT':
            # TOPLEFT: í…Œì´ë¸” ìœ„ì— ìˆëŠ” í…ìŠ¤íŠ¸ëŠ” yê°’ì´ ë” ì‘ìŒ
            text_bottom_y = text_bbox.get('b', 0)

            # í…ìŠ¤íŠ¸ì˜ í•˜ë‹¨ì´ í…Œì´ë¸” ìƒë‹¨ë³´ë‹¤ ìœ„ì— ìˆëŠ”ì§€ í™•ì¸
            if text_bottom_y < table_top_y:
                distance = table_top_y - text_bottom_y
                if distance <= 300:
                    text_content = text_obj.get('text', '').strip()
                    if text_content and len(text_content) > 0:
                        title_candidates.append({
                            'text': text_content,
                            'distance': distance,
                            'y_position': text_bottom_y
                        })

    # ML ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ íƒ€ì´í‹€ ì„ íƒ
    if title_candidates:
        # ë°˜ë³µ íŒ¨í„´ í•„í„°ë§ (ì „ì²˜ë¦¬)
        filtered_candidates = []
        for candidate in title_candidates:
            title_text = candidate['text']
            distance = candidate['distance']

            # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
            if title_text.isdigit():
                continue

            # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸ (2ì ë¯¸ë§Œ)
            if len(title_text.strip()) < 2:
                continue

            # ë°˜ë³µë˜ëŠ” header/footer íŒ¨í„´ ì œì™¸
            if repeated_patterns and title_text in repeated_patterns:
                continue

            # í˜ì´ì§€ í—¤ë” íŒ¨í„´ ì œì™¸
            import re
            if 'â”‚' in title_text or '|' in title_text:
                if re.search(r'\d+\s*[â”‚|]', title_text) or re.search(r'[â”‚|]\s*\d+', title_text):
                    continue
            if re.match(r'^ì œ\d+ì¥', title_text):
                continue

            # ë‹¤ë¥¸ í…Œì´ë¸” ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
            if distance > 50 and all_tables:
                text_bbox = None
                for text_obj in all_texts:
                    if text_obj.get('text', '').strip() == title_text:
                        text_bbox = text_obj.get('prov', [{}])[0].get('bbox', {})
                        break

                if text_bbox and is_text_in_any_table(text_bbox, all_tables, table):
                    continue

            filtered_candidates.append(candidate)

        # ML ëª¨ë¸ ë˜ëŠ” ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ íƒ€ì´í‹€ ì„ íƒ
        if USE_ML_CLASSIFIER:
            return select_best_title_with_model(filtered_candidates)
        else:
            return select_best_title(filtered_candidates)

    return ""


def extract_table_info(table: Dict, all_texts: List[Dict] = None, all_tables: List[Dict] = None, repeated_patterns: set = None) -> Dict:
    """í…Œì´ë¸”ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ"""
    import time
    start = time.time()

    cells = table.get('data', {}).get('table_cells', [])
    page_no = table.get('prov', [{}])[0].get('page_no', -1)

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = [cell.get('text', '').strip() for cell in cells if cell.get('text', '').strip()]

    # í—¤ë” ì…€ ì°¾ê¸°
    # ê° ì—´ë§ˆë‹¤ ê°€ì¥ ê¸´(êµ¬ì²´ì ì¸) í…ìŠ¤íŠ¸ë¥¼ í—¤ë”ë¡œ ì„ íƒ
    header_cells = [cell for cell in cells if cell.get('column_header', False)]

    if header_cells:
        # í—¤ë”ëŠ” ë³´í†µ ìƒë‹¨ ëª‡ ê°œ í–‰ì—ë§Œ ìˆìœ¼ë¯€ë¡œ, ìµœëŒ€ í–‰ ë²ˆí˜¸ í™•ì¸
        max_header_row = max([cell.get('end_row_offset_idx', 0) for cell in header_cells]) if header_cells else 0

        # ìƒë‹¨ 5ê°œ í–‰ ì´ë‚´ì˜ í—¤ë”ë§Œ ì‚¬ìš© (ì¤‘ê°„ ì†Œê³„ í–‰ ë“± ì œì™¸)
        # ë‹¨, ì „ì²´ í—¤ë” í–‰ì´ 5ê°œ ì´í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        header_row_threshold = min(5, max_header_row) if max_header_row > 0 else 5

        # ì—´ë³„ë¡œ í—¤ë” ì…€ ê·¸ë£¹í™”
        headers_by_col = {}
        merged_headers = []  # ë³‘í•©ëœ í—¤ë” ì…€ (ì—¬ëŸ¬ ì—´ì— ê±¸ì¹œ ê²ƒ)

        for cell in header_cells:
            start_col = cell.get('start_col_offset_idx', -1)
            end_col = cell.get('end_col_offset_idx', -1)
            start_row = cell.get('start_row_offset_idx', -1)
            text = cell.get('text', '').strip()

            # ìƒë‹¨ í—¤ë” ì˜ì—­ì˜ ì…€ë§Œ ì‚¬ìš©
            if start_row > header_row_threshold:
                continue

            if text and start_col >= 0:
                span = end_col - start_col

                # ë‹¨ì¼ ì—´ í—¤ë”
                if span == 1:
                    if start_col not in headers_by_col:
                        headers_by_col[start_col] = []
                    headers_by_col[start_col].append(text)
                # ë³‘í•©ëœ í—¤ë” (ì—¬ëŸ¬ ì—´ì— ê±¸ì¹œ ê²ƒ)
                elif span > 1:
                    merged_headers.append({
                        'text': text,
                        'start_col': start_col,
                        'end_col': end_col,
                        'span': span
                    })
                    # ë³‘í•©ëœ í—¤ë”ê°€ í¬í•¨í•˜ëŠ” ê° ì—´ì— ì¶”ê°€
                    for col in range(start_col, end_col):
                        if col not in headers_by_col:
                            headers_by_col[col] = []
                        headers_by_col[col].append(text)

        # ê° ì—´ë§ˆë‹¤ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ë¥¼ í—¤ë”ë¡œ ì„ íƒ
        headers = []
        for col in sorted(headers_by_col.keys()):
            col_headers = headers_by_col[col]
            # ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì„ íƒ (ê°€ì¥ êµ¬ì²´ì ì¸ í—¤ë”)
            longest_header = max(col_headers, key=len)
            headers.append(longest_header)

        # í—¤ë”ê°€ 1ê°œë¿ì´ë©´ í—¤ë”ê°€ ì•„ë‹ ê°€ëŠ¥ì„±ì´ ë†’ìŒ (ë°ì´í„° í–‰ì¼ ìˆ˜ ìˆìŒ)
        if len(headers) == 1:
            headers = []
    else:
        headers = []

    # í‚¤ê°’ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì—´ì˜ ë°ì´í„°, í—¤ë” ì œì™¸)
    key_values = []
    for cell in cells:
        if (not cell.get('column_header', False) and
            not cell.get('row_header', False) and
            cell.get('start_col_offset_idx', -1) == 0 and
            cell.get('text', '').strip()):
            key_values.append(cell.get('text', '').strip())

    # í…Œì´ë¸” êµ¬ì¡° ì •ë³´
    rows = max([cell.get('end_row_offset_idx', 0) for cell in cells]) if cells else 0
    cols = max([cell.get('end_col_offset_idx', 0) for cell in cells]) if cells else 0

    # íƒ€ì´í‹€ ì¶”ì¶œ
    title = ""
    if all_texts:
        title_start = time.time()
        title = find_table_title(table, all_texts, all_tables, repeated_patterns=repeated_patterns)
        title_time = time.time() - title_start
        if title_time > 0.1:
            print(f"      [íƒ€ì´í‹€ ì¶”ì¶œ ëŠë¦¼: {title_time:.2f}ì´ˆ, í˜ì´ì§€ {page_no}]")

    total_time = time.time() - start
    if total_time > 0.5:
        print(f"      [extract_table_info ëŠë¦¼: {total_time:.2f}ì´ˆ, í˜ì´ì§€ {page_no}]")

    return {
        'page_no': page_no,
        'cells': cells,
        'texts': texts,
        'headers': headers,
        'key_values': key_values,
        'rows': rows,
        'cols': cols,
        'bbox': table.get('prov', [{}])[0].get('bbox', {}),
        'title': title,
        'original_table': table
    }


def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” (ë¹„êµìš©)"""
    # ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'\s+', '', text)
    text = re.sub(r'[Â·\-]+$', '', text)  # ëì˜ ì ì„  ì œê±°
    return text.strip()


def is_continuation_text(prev_text: str, curr_text: str) -> bool:
    """ì´ì „ í…ìŠ¤íŠ¸ì™€ í˜„ì¬ í…ìŠ¤íŠ¸ê°€ ì´ì–´ì§€ëŠ”ì§€ í™•ì¸"""
    if not prev_text or not curr_text:
        return False

    prev_norm = normalize_text(prev_text)
    curr_norm = normalize_text(curr_text)

    # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì œì™¸ (ë‹¨ì¼ ë¬¸ìë‚˜ ê¸°í˜¸)
    if len(prev_norm) < 2 or len(curr_norm) < 2:
        return False

    # 1. ìˆ«ì ì—°ì†ì„± ì²´í¬ (ê°€ì¥ ëª…í™•í•œ ì—°ê²°ì„±)
    # ì˜ˆ: "1)" -> "2)", "1." -> "2.", "ê°€." -> "ë‚˜."
    prev_num = re.search(r'(\d+)[).]\s*$', prev_text)
    curr_num = re.search(r'^(\d+)[).]', curr_text)
    if prev_num and curr_num:
        try:
            if int(curr_num.group(1)) == int(prev_num.group(1)) + 1:
                return True
        except:
            pass

    # ê°€ë‚˜ë‹¤ ìˆœì„œ
    prev_hangul = re.search(r'([ê°€-í£])[).]\s*$', prev_text)
    curr_hangul = re.search(r'^([ê°€-í£])[).]', curr_text)
    if prev_hangul and curr_hangul:
        hangul_order = "ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜"
        prev_char = prev_hangul.group(1)
        curr_char = curr_hangul.group(1)
        if prev_char in hangul_order and curr_char in hangul_order:
            if hangul_order.index(curr_char) == hangul_order.index(prev_char) + 1:
                return True

    # 2. ë‹¨ì–´ì˜ ì¼ë¶€ê°€ ì˜ë¦° ê²½ìš° (ë‹¨, ìµœì†Œ 5ì ì´ìƒì˜ ê³µí†µ ë¶€ë¶„)
    # ì˜ˆ: "êµ­ê°€ì¬ë‚œê´€ë¦¬" -> "êµ­ê°€ì¬ë‚œê´€ë¦¬ì‹œìŠ¤í…œ"
    if len(prev_norm) >= 5:
        # ë§ˆì§€ë§‰ 5~15ìê°€ ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì‹œì‘ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€
        for length in range(min(15, len(prev_norm)), 4, -1):
            if curr_norm.startswith(prev_norm[-length:]):
                # ì¶”ê°€ë¡œ ì˜ë¯¸ ìˆëŠ” ì—°ê²°ì¸ì§€ í™•ì¸ (ìµœì†Œ ê¸¸ì´)
                if len(prev_norm[-length:]) >= 5:
                    return True

    # 3. í…Œì´ë¸” row ì—°ì†ì„± (ê°™ì€ ë‹¨ì–´ê°€ ë°˜ë³µë˜ëŠ” íŒ¨í„´)
    # ì˜ˆ: ë§ˆì§€ë§‰ì´ "í•©ê³„" -> ë‹¤ìŒ ì‹œì‘ì´ "í•©ê³„"
    # í•˜ì§€ë§Œ ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ëŠ” ì œì™¸
    common_words = {'í•©ê³„', 'ì†Œê³„', 'ê³„', 'ì´ê³„', 'ë¹„ê³ ', 'ê¸°íƒ€', 'êµ¬ë¶„', 'í•­ëª©', 'ë‚´ìš©',
                    'ë²ˆí˜¸', 'ì—°ë²ˆ', 'ìˆœë²ˆ', '-', 'Â·', 'â€»', 'â—‹', 'â—', 'â–¡', 'â– '}

    # ì¼ë°˜ì ì¸ ë‹¨ì–´ê°€ ì•„ë‹ˆê³ , 3ì ì´ìƒì´ë©°, ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
    if prev_norm == curr_norm and len(prev_norm) >= 3 and prev_norm not in common_words:
        return True

    # 4. ë¬¸ì¥ì´ ì¤‘ê°„ì— ëŠê¸´ ê²½ìš°
    # ë§ˆì§€ë§‰ì´ ì¡°ì‚¬ë‚˜ ë¶ˆì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°
    incomplete_endings = ['ì˜', 'ë¥¼', 'ì„', 'ê°€', 'ì´', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ',
                         'ì™€', 'ê³¼', 'í•˜', 'ë˜', 'í•œ', 'ëœ', 'ë°', 'ë“±', 'ë˜ëŠ”']
    for ending in incomplete_endings:
        if prev_text.strip().endswith(ending) and len(prev_text.strip()) > len(ending) + 2:
            # ë‹¤ìŒ í…ìŠ¤íŠ¸ê°€ ëª…ì‚¬ë‚˜ ë™ì‚¬ë¡œ ì‹œì‘í•˜ë©´ ì—°ê²° ê°€ëŠ¥
            if len(curr_text.strip()) > 2 and not curr_text.strip()[0].isdigit():
                return True

    return False


def has_similar_structure(table1_info: Dict, table2_info: Dict) -> bool:
    """ë‘ í…Œì´ë¸”ì´ ë¹„ìŠ·í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸"""
    # í—¤ë”ê°€ ë¹„ìŠ·í•œì§€ í™•ì¸
    if table1_info['headers'] and table2_info['headers']:
        headers1 = [normalize_text(h) for h in table1_info['headers']]
        headers2 = [normalize_text(h) for h in table2_info['headers']]

        # í—¤ë” ì¼ì¹˜ë„ í™•ì¸
        common = set(headers1) & set(headers2)
        if len(common) / max(len(headers1), len(headers2)) > 0.5:
            return True

        # ë¶€ë¶„ ì¼ì¹˜ í™•ì¸ (í—¤ë” í¬í•¨ ê´€ê³„)
        # ì˜ˆ: "êµ¬ ë¶„"ì´ "êµ¬"ë¥¼ í¬í•¨í•˜ê±°ë‚˜, "ë¯¸ì ìš©ì‹œ ìœ  ë° ëŒ€ì²´"ê°€ "ë¶€ë¶„ì ìš©/ ë¯¸ì ìš©ì‹œ ì‚¬ ìœ  ë° ëŒ€ì²´ ê¸°ìˆ "ì— í¬í•¨
        match_count = 0
        for h1 in headers1:
            for h2 in headers2:
                # ê¸¸ì´ê°€ 2ì ì´ìƒì¸ ê²½ìš°ì—ë§Œ ë¶€ë¶„ ì¼ì¹˜ í™•ì¸
                if len(h1) >= 2 and len(h2) >= 2:
                    if h1 in h2 or h2 in h1:
                        match_count += 1
                        break

        # ìµœì†Œ 2ê°œ ì´ìƒì˜ í—¤ë”ê°€ ë¶€ë¶„ ì¼ì¹˜í•˜ë©´ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ íŒë‹¨
        if match_count >= 2:
            return True

    return True  # êµ¬ì¡° ì •ë³´ë§Œìœ¼ë¡œëŠ” íŒë‹¨í•˜ê¸° ì–´ë ¤ìš°ë©´ True


def calculate_title_similarity(title1: str, title2: str) -> float:
    """ë‘ íƒ€ì´í‹€ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1 ì‚¬ì´ ê°’) - ML ëª¨ë¸ ê¸°ë°˜"""
    if not title1 or not title2:
        return 0.0

    # ì™„ì „ ì¼ì¹˜
    if title1 == title2:
        return 1.0

    if USE_SIMILARITY_MODEL:
        try:
            # Sentence-Transformersë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
            model = get_similarity_model()
            embeddings = model.encode([title1, title2])
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

            # ëª¨ë¸ì´ ê³„ì‚°í•œ ìœ ì‚¬ë„ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í•˜ë“œì½”ë”© ê¸ˆì§€)
            # 0.7 ì´ìƒì´ë©´ ìœ ì‚¬, 0.85 ì´ìƒì´ë©´ ê±°ì˜ ë™ì¼
            if similarity >= 0.85:
                return 1.0
            elif similarity >= 0.7:
                return 0.7
            else:
                return 0.0

        except Exception as e:
            print(f"  âš ï¸  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}, fallback ì‚¬ìš©", flush=True)
            # Fallback: ë‹¨ìˆœ ë¬¸ìì—´ ë¹„êµ
            pass

    # Fallback: ì •ê·œí™” í›„ ë‹¨ìˆœ ë¹„êµ
    t1 = normalize_text(title1)
    t2 = normalize_text(title2)

    if t1 == t2:
        return 1.0
    elif t1 in t2 or t2 in t1:
        return 0.7
    else:
        return 0.0


def _check_text_continuation(table1_info: Dict, table2_info: Dict) -> Tuple[bool, str]:
    """í…ìŠ¤íŠ¸ ì—°ê²°ì„± í™•ì¸ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    if table1_info['texts'] and table2_info['texts']:
        last_texts = table1_info['texts'][-5:]
        first_texts = table2_info['texts'][:5]

        for prev_text in last_texts:
            for curr_text in first_texts:
                if is_continuation_text(prev_text, curr_text):
                    return True, f"í…ìŠ¤íŠ¸ ì—°ê²°: '{prev_text}' -> '{curr_text}'"
    return False, ""


def _check_header_similarity(table1_info: Dict, table2_info: Dict) -> Tuple[bool, str, int, float]:
    """í—¤ë” ìœ ì‚¬ë„ í™•ì¸ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    if table1_info['headers'] and table2_info['headers']:
        headers1 = [normalize_text(h) for h in table1_info['headers'] if len(normalize_text(h)) > 0]
        headers2 = [normalize_text(h) for h in table2_info['headers'] if len(normalize_text(h)) > 0]

        if len(headers1) >= 2 and len(headers2) >= 2:
            headers1_set = set(headers1)
            headers2_set = set(headers2)

            common = headers1_set & headers2_set
            similarity = len(common) / max(len(headers1_set), len(headers2_set))

            partial_match_count = 0
            for h1 in headers1:
                for h2 in headers2:
                    if len(h1) >= 3 and len(h2) >= 3:
                        if len(h1) > len(h2) + 2 and h2 in h1:
                            partial_match_count += 1
                            break
                        elif len(h2) > len(h1) + 2 and h1 in h2:
                            partial_match_count += 1
                            break

            partial_similarity = partial_match_count / max(len(headers1), len(headers2))

            if (similarity >= 1.0 and len(common) >= 2) or (partial_similarity >= 0.5 and partial_match_count >= 2):
                return True, "", len(common), partial_match_count

    return False, "", 0, 0


def _check_width_similarity(table1_info: Dict, table2_info: Dict) -> Tuple[bool, float, float, float]:
    """í…Œì´ë¸” ë„ˆë¹„ ìœ ì‚¬ë„ í™•ì¸ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    bbox1 = table1_info['bbox']
    bbox2 = table2_info['bbox']

    if bbox1 and bbox2:
        width1 = abs(bbox1.get('r', 0) - bbox1.get('l', 0))
        width2 = abs(bbox2.get('r', 0) - bbox2.get('l', 0))

        if width1 > 0 and width2 > 0:
            width_diff_ratio = abs(width1 - width2) / min(width1, width2)
            return True, width1, width2, width_diff_ratio

    return False, 0, 0, 0


def check_table_connection(table1_info: Dict, table2_info: Dict) -> Tuple[bool, str]:
    """ë‘ í…Œì´ë¸”ì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ì¡°ê±´ ë³‘ë ¬ ì²˜ë¦¬)"""
    # í˜ì´ì§€ ì°¨ì´ í™•ì¸
    page_diff = table2_info['page_no'] - table1_info['page_no']

    # í˜ì´ì§€ê°€ ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆìœ¼ë©´ ì—°ê²° ì•ˆë¨
    if page_diff < 0 or page_diff > 2:
        return False, "í˜ì´ì§€ê°€ ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆìŒ"

    # êµ¬ì¡° ìœ ì‚¬ì„± í™•ì¸
    if not has_similar_structure(table1_info, table2_info):
        return False, "í…Œì´ë¸” êµ¬ì¡°ê°€ ë‹¤ë¦„"

    # íƒ€ì´í‹€ ì •ë³´
    title1 = table1_info.get('title', '')
    title2 = table2_info.get('title', '')

    # ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  ì¡°ê±´ë“¤
    from concurrent.futures import ThreadPoolExecutor

    results = {}
    futures = {}

    with ThreadPoolExecutor(max_workers=4) as executor:
        # ëª¨ë“  íƒœìŠ¤í¬ë¥¼ ë¨¼ì € ì œì¶œ (ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘)
        if title1 and title2:
            futures['title'] = executor.submit(calculate_title_similarity, title1, title2)

        futures['text'] = executor.submit(_check_text_continuation, table1_info, table2_info)
        futures['header'] = executor.submit(_check_header_similarity, table1_info, table2_info)
        futures['width'] = executor.submit(_check_width_similarity, table1_info, table2_info)

        # ëª¨ë“  ê²°ê³¼ë¥¼ í•œë²ˆì— ìˆ˜ì§‘ (ë³‘ë ¬ ì‹¤í–‰ ì™„ë£Œ ëŒ€ê¸°)
        if 'title' in futures:
            results['title_similarity'] = futures['title'].result()

        results['text_connected'], results['text_reason'] = futures['text'].result()
        results['header_similar'], _, results['header_common_count'], results['header_partial_count'] = futures['header'].result()
        results['width_available'], results['width1'], results['width2'], results['width_diff_ratio'] = futures['width'].result()

    # === íƒ€ì´í‹€ ì²´í¬ (í…ìŠ¤íŠ¸ ì—°ê²°ë³´ë‹¤ ìš°ì„ ) ===
    if title1 and title2:
        title_similarity = results.get('title_similarity', 0)

        # ê°™ì€ í˜ì´ì§€ì— ìˆìœ¼ë©´ì„œ íƒ€ì´í‹€ì´ ë‹¤ë¥¸ ê²½ìš°
        if page_diff == 0 and title_similarity < 0.8:
            return False, f"ê°™ì€ í˜ì´ì§€ì˜ ë‹¤ë¥¸ í…Œì´ë¸” ('{title1}' vs '{title2}', ìœ ì‚¬ë„: {title_similarity:.2f})"

        # ë‹¤ë¥¸ í˜ì´ì§€ì¸ ê²½ìš° ìœ ì‚¬ë„ 85% ë¯¸ë§Œì¼ ë•Œë§Œ ë¶„ë¦¬
        if title_similarity < 0.85:
            return False, f"íƒ€ì´í‹€ì´ ë‹¤ë¦„ ('{title1}' vs '{title2}', ìœ ì‚¬ë„: {title_similarity:.2f})"

    # ë‘ ë²ˆì§¸ í…Œì´ë¸”ì—ë§Œ íƒ€ì´í‹€ì´ ìˆëŠ” ê²½ìš°
    elif not title1 and title2:
        return False, f"ë‘ ë²ˆì§¸ í…Œì´ë¸”ì— ìƒˆë¡œìš´ íƒ€ì´í‹€ ì‹œì‘ ('{title2}')"

    # === í…ìŠ¤íŠ¸ ì—°ê²°ì„± í™•ì¸ ===
    # ì£¼ì˜: í…ìŠ¤íŠ¸ ì—°ê²°ì„±ë§Œìœ¼ë¡œëŠ” ë¶ˆì¶©ë¶„ (ìš°ì—°íˆ ë¹„ìŠ·í•œ ë‹¨ì–´ë¡œ ì‹œì‘/ëë‚  ìˆ˜ ìˆìŒ)
    # íƒ€ì´í‹€ì´ë‚˜ í—¤ë” ë“± ë” ëª…í™•í•œ ì¦ê±°ê°€ í•„ìš”
    # if results['text_connected']:
    #     return True, results['text_reason']

    # === íƒ€ì´í‹€ ìˆìŒ â†’ íƒ€ì´í‹€ ì—†ìŒ ì—°ê²° ì²´í¬ ===
    # ì£¼ì˜: ì´ ì¡°ê±´ì€ ë§¤ìš° ì‹ ì¤‘í•˜ê²Œ ì ìš©í•´ì•¼ í•¨
    # íƒ€ì´í‹€ì´ ìˆëŠ” í…Œì´ë¸” ë’¤ì— íƒ€ì´í‹€ ì—†ëŠ” í…Œì´ë¸”ì´ ì™€ë„, ëª…í™•í•œ ì—°ê²° ì¦ê±°ê°€ ìˆì–´ì•¼ ë³‘í•©
    # ë‹¨ìˆœíˆ êµ¬ì¡°ê°€ ìœ ì‚¬í•˜ë‹¤ê³  ë³‘í•©í•˜ë©´ ì•ˆ ë¨ (ë³„ê°œì˜ í…Œì´ë¸”ì¼ ìˆ˜ ìˆìŒ)
    # ì´ ì¡°ê±´ì€ ì œê±°í•˜ê±°ë‚˜ ë§¤ìš° ì œí•œì ìœ¼ë¡œë§Œ ì‚¬ìš©
    # if title1 and not title2 and page_diff == 1:
    #     if has_similar_structure(table1_info, table2_info):
    #         return True, f"íƒ€ì´í‹€ì´ ìˆëŠ” í…Œì´ë¸” ë’¤ ê³„ì† (êµ¬ì¡° ìœ ì‚¬, ì—°ì† í˜ì´ì§€)"

    # === í—¤ë” ë™ì¼ ì²´í¬ ===
    if results['header_similar']:
        # ë‘ ë²ˆì§¸ í…Œì´ë¸”ì—ë§Œ íƒ€ì´í‹€ì´ ìˆìœ¼ë©´ ë¶„ë¦¬
        if not title1 and title2:
            return False, f"ë‘ ë²ˆì§¸ í…Œì´ë¸”ì— ìƒˆë¡œìš´ íƒ€ì´í‹€ ì‹œì‘ ('{title2}')"

        # í‚¤ê°’ ì¤‘ë³µ ì²´í¬
        keys1 = set([normalize_text(k) for k in table1_info['key_values'][:10]])
        keys2 = set([normalize_text(k) for k in table2_info['key_values'][:10]])

        if keys1 and keys2:
            common_keys = keys1 & keys2
            key_overlap_ratio = len(common_keys) / min(len(keys1), len(keys2)) if min(len(keys1), len(keys2)) > 0 else 0

            if key_overlap_ratio > 0.3:
                return False, f"í—¤ë”ëŠ” ë™ì¼í•˜ì§€ë§Œ í‚¤ê°’ì´ ê²¹ì¹¨ ({len(common_keys)}ê°œ ì¤‘ë³µ)"

        if results['header_common_count'] >= 2:
            return True, f"í—¤ë”ê°€ ë™ì¼í•¨ ({results['header_common_count']}ê°œ ì¼ì¹˜)"
        else:
            return True, f"í—¤ë”ê°€ ë¶€ë¶„ ì¼ì¹˜í•¨ ({results['header_partial_count']}ê°œ ë¶€ë¶„ ì¼ì¹˜)"

    # === íƒ€ì´í‹€ ìœ ì‚¬ë„ í™•ì¸ (í—¤ë” ì²´í¬ ì´í›„) ===
    if title1 and title2:
        title_similarity = results.get('title_similarity', 0)

        if title_similarity >= 1.0:
            return True, f"íƒ€ì´í‹€ì´ ê°™ìŒ ('{title1}')"

        if title_similarity < 0.85:
            return False, f"íƒ€ì´í‹€ì´ ë‹¤ë¦„ ('{title1}' vs '{title2}', ìœ ì‚¬ë„: {title_similarity:.2f})"

    elif not title1 and title2:
        return False, f"ë‘ ë²ˆì§¸ í…Œì´ë¸”ì— ìƒˆë¡œìš´ íƒ€ì´í‹€ ì‹œì‘ ('{title2}')"

    # === í—¤ë” ìˆìŒ -> í—¤ë” ì—†ìŒ íŒ¨í„´ ===
    if table1_info['headers'] and not table2_info['headers']:
        if page_diff >= 1 and page_diff <= 2:
            # í…ìŠ¤íŠ¸ ì—°ê²°ì„±ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ë„ˆë¬´ ë§ì€ ì˜¤íƒ)
            # if results['text_connected']:
            #     return True, f"í—¤ë” í…Œì´ë¸” ë’¤ ë°ì´í„° í…Œì´ë¸” - {results['text_reason']}"
            pass

            # íƒ€ì´í‹€ ìˆëŠ” í—¤ë” í…Œì´ë¸” ë’¤ ë°ì´í„° í…Œì´ë¸”
            # ì¡°ê±´: íƒ€ì´í‹€ì´ ìˆê³ , í—¤ë”ê°€ ìˆê³ , ë‹¤ìŒ í…Œì´ë¸”ì— í—¤ë”ê°€ ì—†ê³ , ì—´ ê°œìˆ˜ì™€ ë„ˆë¹„ê°€ ìœ ì‚¬í•œ ê²½ìš°
            if title1 and page_diff == 1:
                col_diff = abs(table1_info['cols'] - table2_info['cols'])

                # ì—´ ê°œìˆ˜ì™€ ë„ˆë¹„ë„ í•¨ê»˜ ì²´í¬
                if results['width_available']:
                    width_diff_ratio = results['width_diff_ratio']

                    # ì—´ ê°œìˆ˜ê°€ ê°™ê±°ë‚˜ ì°¨ì´ê°€ 1 ì´í•˜ì´ê³ , ë„ˆë¹„ ì°¨ì´ê°€ 20% ì´ë‚´ë©´ ì—°ê²°
                    if col_diff <= 1 and table1_info['cols'] >= 2 and width_diff_ratio <= 0.2:
                        return True, f"íƒ€ì´í‹€ì´ ìˆëŠ” í—¤ë” í…Œì´ë¸” ë’¤ ë°ì´í„° í…Œì´ë¸” (ì—´ {table1_info['cols']}ê°œ vs {table2_info['cols']}ê°œ, ë„ˆë¹„ ìœ ì‚¬ë„ {(1-width_diff_ratio)*100:.0f}%, ì—°ì† í˜ì´ì§€)"

            # ì—´ ê°œìˆ˜ì™€ ë„ˆë¹„ ì²´í¬ (íƒ€ì´í‹€ ì—†ëŠ” ê²½ìš°)
            # ì¡°ê±´: í—¤ë” ìˆëŠ” í…Œì´ë¸” ë’¤ì— í—¤ë” ì—†ëŠ” í…Œì´ë¸”ì´ ì—°ì†ìœ¼ë¡œ ì˜¤ëŠ” ê²½ìš°
            if page_diff == 1:
                col_diff = abs(table1_info['cols'] - table2_info['cols'])

                if results['width_available']:
                    width_diff_ratio = results['width_diff_ratio']

                    # ì˜µì…˜ 1: ì—´ ê°œìˆ˜ ì •í™•íˆ ì¼ì¹˜, ë„ˆë¹„ ì°¨ì´ 10% ì´ë‚´
                    if col_diff == 0 and table1_info['cols'] >= 2 and width_diff_ratio <= 0.1:
                        return True, f"í—¤ë” í…Œì´ë¸” ë’¤ ë°ì´í„° í…Œì´ë¸” (ì—´ {table1_info['cols']}ê°œ ë™ì¼, ë„ˆë¹„ ìœ ì‚¬ë„ {(1-width_diff_ratio)*100:.0f}%, ì—°ì† í˜ì´ì§€)"

                    # ì˜µì…˜ 2: ì—´ ê°œìˆ˜ ì°¨ì´ 1, ë„ˆë¹„ ê±°ì˜ ë™ì¼ (1% ì´ë‚´)
                    # ê°™ì€ í…Œì´ë¸”ì´ í˜ì´ì§€ ë„˜ê¹€ìœ¼ë¡œ ì¸í•´ ì—´ì´ ì•½ê°„ ë‹¤ë¥´ê²Œ ì¸ì‹ë  ìˆ˜ ìˆìŒ
                    if col_diff == 1 and table1_info['cols'] >= 2 and width_diff_ratio <= 0.01:
                        return True, f"í—¤ë” í…Œì´ë¸” ë’¤ ë°ì´í„° í…Œì´ë¸” (ì—´ {table1_info['cols']}ê°œ vs {table2_info['cols']}ê°œ, ë„ˆë¹„ ê±°ì˜ ë™ì¼ {(1-width_diff_ratio)*100:.1f}%, ì—°ì† í˜ì´ì§€)"

    # === ë‘˜ ë‹¤ í—¤ë” ì—†ìŒ íŒ¨í„´ ===
    if not table1_info['headers'] and not table2_info['headers']:
        if not title1 and not title2 and page_diff == 1:
            col_diff = abs(table1_info['cols'] - table2_info['cols'])

            if results['width_available']:
                width_diff_ratio = results['width_diff_ratio']

                if width_diff_ratio > 0.3:
                    return False, f"í…Œì´ë¸” ë„ˆë¹„ ì°¨ì´ê°€ í¼ (ë„ˆë¹„ {results['width1']:.0f} vs {results['width2']:.0f}, ì°¨ì´ {width_diff_ratio*100:.0f}%)"

                # í—¤ë” ì—†ëŠ” ë°ì´í„° í…Œì´ë¸”ë¼ë¦¬ëŠ” ì—´ ê°œìˆ˜ê°€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨
                if col_diff == 0 and table1_info['cols'] >= 2:
                    # í‚¤ê°’ ì¤‘ë³µ ì²´í¬
                    keys1 = set([normalize_text(k) for k in table1_info['key_values'][:10]])
                    keys2 = set([normalize_text(k) for k in table2_info['key_values'][:10]])

                    if keys1 and keys2:
                        common_keys = keys1 & keys2
                        key_overlap_ratio = len(common_keys) / min(len(keys1), len(keys2)) if min(len(keys1), len(keys2)) > 0 else 0

                        if key_overlap_ratio > 0.5:
                            return False, f"ê°™ì€ í˜•ì‹ì˜ ë°˜ë³µ í…Œì´ë¸” (í‚¤ê°’ {len(common_keys)}ê°œ ì¤‘ë³µ, ì¤‘ë³µë¥  {key_overlap_ratio:.0%})"

                    return True, f"í—¤ë” ì—†ëŠ” ë°ì´í„° í…Œì´ë¸” ì—°ì† (ì—´ {table1_info['cols']}ê°œ ë™ì¼, ì—°ì† í˜ì´ì§€)"

    return False, "ì—°ê²° ì¡°ê±´ ë¯¸ì¶©ì¡±"


def merge_tables(table_group: List[Dict]) -> Dict:
    """ì—°ê²°ëœ í…Œì´ë¸”ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©"""
    if not table_group:
        return None

    if len(table_group) == 1:
        return table_group[0]['original_table']

    # ì²« ë²ˆì§¸ í…Œì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ ë³‘í•©
    merged = table_group[0]['original_table'].copy()
    merged['merged_from_pages'] = [t['page_no'] for t in table_group]

    # ëª¨ë“  ì…€ í•©ì¹˜ê¸°
    all_cells = []
    for table_info in table_group:
        all_cells.extend(table_info['cells'])

    merged['data']['table_cells'] = all_cells

    return merged


def _check_connection_pair(args):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í…Œì´ë¸” ìŒ ë¹„êµ í•¨ìˆ˜"""
    table_info_i, table_info_j, i, j = args
    is_connected, reason = check_table_connection(table_info_i, table_info_j)
    return (i, j, is_connected, reason)


def find_connected_table_groups(tables: List[Dict], all_texts: List[Dict] = None, repeated_patterns: set = None, use_parallel: bool = True, max_workers: int = None) -> List[List[int]]:
    """ì—°ê²°ëœ í…Œì´ë¸” ê·¸ë£¹ ì°¾ê¸°

    Args:
        tables: í…Œì´ë¸” ëª©ë¡
        all_texts: ì „ì²´ í…ìŠ¤íŠ¸ ëª©ë¡
        repeated_patterns: ë°˜ë³µë˜ëŠ” íŒ¨í„´
        use_parallel: ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)
    """
    print(f"  [1/4] í…Œì´ë¸” ì •ë³´ ì¶”ì¶œ ì¤‘... ({len(tables)}ê°œ)", flush=True)

    # íƒ€ì´í‹€ ì¶”ì¶œì€ ìˆœì°¨ ì²˜ë¦¬ (ML ëª¨ë¸ ê³µìœ ë¥¼ ìœ„í•´)
    # ë³‘ë ¬ ì²˜ë¦¬í•˜ë©´ ê° í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ëª¨ë¸ì„ ë¡œë“œí•´ì„œ ì˜¤íˆë ¤ ëŠë¦¼
    table_infos = []
    for idx, table in enumerate(tables):
        if (idx + 1) % 10 == 0 or (idx + 1) == len(tables):
            print(f"      â†’ ì§„í–‰: {idx + 1}/{len(tables)} í…Œì´ë¸” ì²˜ë¦¬ ì¤‘", end='\r', flush=True)
        table_infos.append(extract_table_info(table, all_texts, tables, repeated_patterns))

    print(f"\n  [1/4] ì™„ë£Œ - íƒ€ì´í‹€ ì¶”ì¶œë¨", flush=True)

    # í˜ì´ì§€ ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
    print(f"  [2/4] í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ ì¤‘...", flush=True)
    sorted_indices = sorted(range(len(table_infos)),
                          key=lambda i: table_infos[i]['page_no'])

    visited = set()
    groups = []
    connection_reasons = []
    all_disconnection_reasons = {}  # ëª¨ë“  ë¹„ì—°ì† ì´ìœ  ì €ì¥ (í…Œì´ë¸” ì¸ë±ìŠ¤ë³„)

    # ë¨¼ì € ëª¨ë“  ì¸ì ‘ í…Œì´ë¸” ìŒì— ëŒ€í•´ ì—°ì†ì„± ì²´í¬
    connection_results = {}

    print(f"  [3/4] í…Œì´ë¸” ì—°ê²°ì„± ì²´í¬ ì¤‘... ({len(sorted_indices)-1}ê°œ ìŒ)", flush=True)
    if use_parallel and len(sorted_indices) > 1:
        # ë³‘ë ¬ ì²˜ë¦¬: ëª¨ë“  ì¸ì ‘ ìŒì„ ë™ì‹œì— ë¹„êµ
        comparison_tasks = []
        for idx in range(len(sorted_indices) - 1):
            i = sorted_indices[idx]
            j = sorted_indices[idx + 1]
            comparison_tasks.append((table_infos[i], table_infos[j], i, j))

        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰ (ProcessPoolë³´ë‹¤ ì˜¤ë²„í—¤ë“œ ì ìŒ)
        print(f"      â†’ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ (ì›Œì»¤: {max_workers or 'CPU ì½”ì–´ ìˆ˜'})", flush=True)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(_check_connection_pair, task) for task in comparison_tasks]

            completed = 0
            total = len(futures)
            for future in as_completed(futures):
                i, j, is_connected, reason = future.result()
                connection_results[(i, j)] = (is_connected, reason)

                completed += 1
                if completed % 10 == 0 or completed == total:
                    print(f"      â†’ ì§„í–‰: {completed}/{total} ìŒ ì™„ë£Œ", end='\r', flush=True)

                if not is_connected:
                    if i not in all_disconnection_reasons:
                        all_disconnection_reasons[i] = []
                    all_disconnection_reasons[i].append(f"Table {i} -X-> {j}: {reason}")
            print(flush=True)  # ì¤„ë°”ê¿ˆ
    else:
        # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
        print(f"      â†’ ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")
        for idx in range(len(sorted_indices) - 1):
            i = sorted_indices[idx]
            j = sorted_indices[idx + 1]

            is_connected, reason = check_table_connection(
                table_infos[i],
                table_infos[j]
            )

            connection_results[(i, j)] = (is_connected, reason)

            if not is_connected:
                # ë¹„ì—°ì†ì¸ ê²½ìš° ì €ì¥
                if i not in all_disconnection_reasons:
                    all_disconnection_reasons[i] = []
                all_disconnection_reasons[i].append(f"Table {i} -X-> {j}: {reason}")

    print(f"  [3/4] ì™„ë£Œ - ì—°ê²°ì„± ì²´í¬ ì™„ë£Œ", flush=True)

    # ì—°ê²°ëœ ê·¸ë£¹ ì°¾ê¸°
    print(f"  [4/4] í…Œì´ë¸” ê·¸ë£¹í•‘ ì¤‘...", flush=True)
    for i in sorted_indices:
        if i in visited:
            continue

        current_group = [i]
        current_reasons = []
        visited.add(i)

        # ìˆœì°¨ì ìœ¼ë¡œ ë°”ë¡œ ë‹¤ìŒ í…Œì´ë¸”ë§Œ í™•ì¸ (ê±´ë„ˆë›°ê¸° ë°©ì§€)
        current_idx = i
        current_pos = sorted_indices.index(i)

        for j_pos in range(current_pos + 1, len(sorted_indices)):
            j = sorted_indices[j_pos]

            if j in visited:
                continue

            # ì´ë¯¸ ê³„ì‚°ëœ ì—°ê²° ê²°ê³¼ ì‚¬ìš©
            is_connected, reason = connection_results.get((current_idx, j), (False, "ë¹„êµë˜ì§€ ì•ŠìŒ"))

            if is_connected:
                current_group.append(j)
                current_reasons.append(f"Table {current_idx} -> {j}: {reason}")
                visited.add(j)
                current_idx = j  # ë‹¤ìŒ ë¹„êµë¥¼ ìœ„í•´ í˜„ì¬ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
            else:
                # ì—°ê²°ë˜ì§€ ì•Šìœ¼ë©´ ê·¸ë£¹ ì¢…ë£Œ
                break

        groups.append(current_group)
        connection_reasons.append(current_reasons)

    print(f"  [4/4] ì™„ë£Œ - {len([g for g in groups if len(g) > 1])}ê°œ ê·¸ë£¹ ìƒì„±", flush=True)

    return groups, connection_reasons, all_disconnection_reasons, table_infos


def create_visualization_pdf(json_files: List[str], output_dir: str, font_name: str):
    """ë³‘í•© ê²°ê³¼ë¥¼ ì‹œê°í™”í•œ PDF ìƒì„±"""
    pdf_path = os.path.join(output_dir, "merged_tables_visualization.pdf")
    doc = SimpleDocTemplate(pdf_path, pagesize=A4)
    story = []

    styles = getSampleStyleSheet()
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontName=font_name,
        fontSize=16,
        textColor=colors.HexColor('#2C3E50'),
        spaceAfter=30,
        alignment=TA_CENTER
    )

    heading_style = ParagraphStyle(
        'CustomHeading',
        parent=styles['Heading2'],
        fontName=font_name,
        fontSize=12,
        textColor=colors.HexColor('#34495E'),
        spaceAfter=12,
        spaceBefore=12
    )

    normal_style = ParagraphStyle(
        'CustomNormal',
        parent=styles['Normal'],
        fontName=font_name,
        fontSize=9,
        leading=12
    )

    # ì œëª©
    story.append(Paragraph("í…Œì´ë¸” ë³‘í•© ë¶„ì„ ê²°ê³¼", title_style))
    story.append(Spacer(1, 0.5*cm))

    total_merged = 0

    for json_file in json_files:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        filename = os.path.basename(json_file)
        story.append(Paragraph(f"íŒŒì¼: {filename}", heading_style))

        tables = data.get('tables', [])
        groups, reasons, all_disconnections, table_infos = find_connected_table_groups(tables)

        # ë³‘í•©ëœ ê·¸ë£¹ê³¼ ë‹¨ì¼ ê·¸ë£¹ ë¶„ë¦¬
        merged_groups = [g for g in groups if len(g) > 1]
        merged_reasons = [r for g, r in zip(groups, reasons) if len(g) > 1]
        single_groups = [g for g in groups if len(g) == 1]

        total_merged += len(merged_groups)

        for group_idx, (group, group_reasons) in enumerate(zip(merged_groups, merged_reasons), 1):
            # ê·¸ë£¹ ì •ë³´
            pages = [table_infos[i]['page_no'] for i in group]
            story.append(Paragraph(
                f"ë³‘í•© ê·¸ë£¹ {group_idx}: {len(group)}ê°œ í…Œì´ë¸” (í˜ì´ì§€ {pages})",
                normal_style
            ))

            # íƒ€ì´í‹€ ì •ë³´ í‘œì‹œ
            titles = [table_infos[i]['title'] for i in group if table_infos[i]['title']]
            if titles:
                # ì²« ë²ˆì§¸ íƒ€ì´í‹€ (ëŒ€í‘œ íƒ€ì´í‹€)
                story.append(Paragraph(f"  ğŸ“‹ íƒ€ì´í‹€: {titles[0]}", normal_style))

            # ì—°ê²° ì´ìœ 
            for reason in group_reasons:
                story.append(Paragraph(f"  â€¢ {reason}", normal_style))

            # ë¹„ì—°ì† ì´ìœ  (ì´ ê·¸ë£¹ì˜ í…Œì´ë¸”ë“¤ê³¼ ê´€ë ¨ëœ ê²ƒ)
            for table_idx in group:
                if table_idx in all_disconnections:
                    for reason in all_disconnections[table_idx]:
                        story.append(Paragraph(f"  â€¢ (ë¹„ì—°ì†) {reason}", normal_style))

            # í…Œì´ë¸” ë¯¸ë¦¬ë³´ê¸° ë°ì´í„° ì¤€ë¹„
            preview_data = [["í…Œì´ë¸”", "í˜ì´ì§€", "í–‰ìˆ˜", "ì—´ìˆ˜", "íƒ€ì´í‹€", "ìƒ˜í”Œ í…ìŠ¤íŠ¸"]]

            for idx in group:
                info = table_infos[idx]
                sample_text = info['texts'][0][:20] + "..." if info['texts'] else ""
                title_text = info['title'][:30] + "..." if info['title'] and len(info['title']) > 30 else (info['title'] or "")
                preview_data.append([
                    f"#{idx}",
                    str(info['page_no']),
                    str(info['rows']),
                    str(info['cols']),
                    title_text,
                    sample_text
                ])

            # í…Œì´ë¸” ìŠ¤íƒ€ì¼ (íƒ€ì´í‹€ ì»¬ëŸ¼ ì¶”ê°€ë¡œ ë„ˆë¹„ ì¡°ì •)
            t = Table(preview_data, colWidths=[1.5*cm, 1.5*cm, 1.5*cm, 1.5*cm, 5*cm, 5*cm])
            t.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#3498DB')),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, -1), font_name),
                ('FONTSIZE', (0, 0), (-1, 0), 10),
                ('FONTSIZE', (0, 1), (-1, -1), 8),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                ('VALIGN', (0, 0), (-1, -1), 'TOP'),
            ]))

            story.append(Spacer(1, 0.2*cm))
            story.append(t)
            story.append(Spacer(1, 0.5*cm))

        # ë‹¨ì¼ í…Œì´ë¸” ì •ë³´ í‘œì‹œ
        if single_groups:
            story.append(Spacer(1, 0.5*cm))
            story.append(Paragraph(f"ë‹¨ì¼ í…Œì´ë¸” ({len(single_groups)}ê°œ)", normal_style))
            story.append(Spacer(1, 0.3*cm))

            # ë‹¨ì¼ í…Œì´ë¸” ë¯¸ë¦¬ë³´ê¸°
            single_preview_data = [["í…Œì´ë¸”", "í˜ì´ì§€", "í–‰ìˆ˜", "ì—´ìˆ˜", "íƒ€ì´í‹€", "ìƒ˜í”Œ í…ìŠ¤íŠ¸"]]

            for group in single_groups[:20]:  # ìµœëŒ€ 20ê°œë§Œ í‘œì‹œ
                idx = group[0]
                info = table_infos[idx]
                sample_text = info['texts'][0][:20] + "..." if info['texts'] else ""
                title_text = info['title'][:30] + "..." if info['title'] and len(info['title']) > 30 else (info['title'] or "")
                single_preview_data.append([
                    f"#{idx}",
                    str(info['page_no']),
                    str(info['rows']),
                    str(info['cols']),
                    title_text,
                    sample_text
                ])

            t_single = Table(single_preview_data, colWidths=[1.5*cm, 1.5*cm, 1.5*cm, 1.5*cm, 5*cm, 5*cm])
            t_single.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#95A5A6')),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, -1), font_name),
                ('FONTSIZE', (0, 0), (-1, 0), 10),
                ('FONTSIZE', (0, 1), (-1, -1), 8),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.lightgrey),
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                ('VALIGN', (0, 0), (-1, -1), 'TOP'),
            ]))

            story.append(t_single)
            if len(single_groups) > 20:
                story.append(Paragraph(f"  (ì™¸ {len(single_groups) - 20}ê°œ ìƒëµ...)", normal_style))

        story.append(PageBreak())

    # ìš”ì•½ í˜ì´ì§€
    story.append(Paragraph("ìš”ì•½", title_style))
    story.append(Paragraph(f"ì´ ë³‘í•©ëœ ê·¸ë£¹ ìˆ˜: {total_merged}", normal_style))

    doc.build(story)
    return pdf_path


def process_json_files(input_dir: str, output_dir: str, original_json_dir: str = "output"):
    """JSON íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë³‘í•©ëœ í…Œì´ë¸” ìƒì„±"""
    os.makedirs(output_dir, exist_ok=True)

    # í•œê¸€ í°íŠ¸ ë“±ë¡
    font_name = register_korean_font()

    json_files = list(Path(input_dir).glob('*.json'))

    all_results = []

    for file_idx, json_file in enumerate(json_files, 1):
        print(f"\n" + "="*60, flush=True)
        print(f"[{file_idx}/{len(json_files)}] ì²˜ë¦¬ ì¤‘: {json_file.name}", flush=True)
        print("="*60, flush=True)

        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        tables = data.get('tables', [])
        print(f"  ì´ í…Œì´ë¸” ìˆ˜: {len(tables)}")

        # ì›ë³¸ JSONì—ì„œ í…ìŠ¤íŠ¸ ì •ë³´ ë¡œë“œ
        all_texts = []
        source_file = data.get('source_file', '')
        if source_file:
            original_json_path = os.path.join(original_json_dir, os.path.basename(source_file))
            if os.path.exists(original_json_path):
                try:
                    with open(original_json_path, 'r', encoding='utf-8') as f:
                        original_data = json.load(f)
                        all_texts = original_data.get('texts', [])
                        print(f"  ì›ë³¸ JSONì—ì„œ {len(all_texts)}ê°œ í…ìŠ¤íŠ¸ ë¡œë“œ")
                except Exception as e:
                    print(f"  ê²½ê³ : ì›ë³¸ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ë°˜ë³µë˜ëŠ” header/footer íŒ¨í„´ ê°ì§€
        repeated_patterns = set()
        if all_texts:
            repeated_patterns = detect_repeated_headers_footers(all_texts, min_pages=3)
            if repeated_patterns:
                print(f"  ê°ì§€ëœ ë°˜ë³µ íŒ¨í„´ {len(repeated_patterns)}ê°œ: {list(repeated_patterns)[:5]}")

        # ì—°ê²°ëœ í…Œì´ë¸” ê·¸ë£¹ ì°¾ê¸°
        groups, reasons, all_disconnections, table_infos = find_connected_table_groups(tables, all_texts, repeated_patterns)

        # ë³‘í•©ëœ ê·¸ë£¹ë§Œ í•„í„°ë§
        merged_groups = [g for g in groups if len(g) > 1]
        merged_reasons = [r for g, r in zip(groups, reasons) if len(g) > 1]

        print(f"  ë³‘í•©ëœ ê·¸ë£¹ ìˆ˜: {len(merged_groups)}")
        print(f"  ë‹¨ì¼ í…Œì´ë¸” ìˆ˜: {len(tables) - sum(len(g) for g in merged_groups)}")

        # ë‹¨ì¼ í…Œì´ë¸” ê·¸ë£¹ (ì—°ê²°ë˜ì§€ ì•Šì€ í…Œì´ë¸”ë“¤)
        single_table_groups = []
        for g_idx, (group, group_reasons) in enumerate(zip(groups, reasons)):
            if len(group) == 1:
                table_idx = group[0]
                disconnection_reason = ""
                # ì´ í…Œì´ë¸”ì˜ ë¹„ì—°ì† ì´ìœ  ì°¾ê¸°
                if table_idx in all_disconnections:
                    disconnection_reason = all_disconnections[table_idx][0] if all_disconnections[table_idx] else ""

                single_table_groups.append({
                    'table_idx': table_idx,
                    'page_no': table_infos[table_idx]['page_no'],
                    'title': table_infos[table_idx]['title'],
                    'disconnection_reason': disconnection_reason
                })

        # ê²°ê³¼ ì €ì¥
        result = {
            'source_file': str(json_file),
            'original_table_count': len(tables),
            'merged_groups': [],
            'single_tables': single_table_groups,  # ë‹¨ì¼ í…Œì´ë¸” ì •ë³´
            'all_disconnections': all_disconnections,  # ëª¨ë“  ë¹„ì—°ì† ì •ë³´ ì €ì¥
            'table_infos': [  # ëª¨ë“  í…Œì´ë¸” ì •ë³´ ì €ì¥ (íƒ€ì´í‹€ í¬í•¨)
                {
                    'table_idx': idx,
                    'page_no': info['page_no'],
                    'title': info['title']
                }
                for idx, info in enumerate(table_infos)
            ]
        }

        for group_idx, (group, group_reasons) in enumerate(zip(merged_groups, merged_reasons)):
            # ì´ ê·¸ë£¹ì— ì†í•œ í…Œì´ë¸”ë“¤ì˜ ë¹„ì—°ì† ì´ìœ  ìˆ˜ì§‘
            group_disconnections = []
            for table_idx in group:
                if table_idx in all_disconnections:
                    group_disconnections.extend(all_disconnections[table_idx])

            group_info = {
                'group_id': group_idx,
                'table_indices': group,
                'pages': [table_infos[i]['page_no'] for i in group],
                'connection_reasons': group_reasons,
                'disconnection_reasons': group_disconnections,
                'merged_table': merge_tables([table_infos[i] for i in group])
            }
            result['merged_groups'].append(group_info)

            print(f"\n  ê·¸ë£¹ {group_idx + 1}:")
            print(f"    í…Œì´ë¸” ì¸ë±ìŠ¤: {group}")
            print(f"    í˜ì´ì§€: {group_info['pages']}")
            # íƒ€ì´í‹€ ì¶œë ¥
            for idx in group:
                if table_infos[idx]['title']:
                    print(f"    í…Œì´ë¸” {idx} íƒ€ì´í‹€: {table_infos[idx]['title']}")
            for reason in group_reasons:
                print(f"    - {reason}")
            if group_disconnections:
                for reason in group_disconnections:
                    print(f"    - (ë¹„ì—°ì†) {reason}")

        all_results.append(result)

        # ê°œë³„ íŒŒì¼ ê²°ê³¼ ì €ì¥
        # Windowsì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
        safe_filename = json_file.stem.replace('+', '_').replace(':', '_').replace('?', '_').replace('*', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
        output_file = os.path.join(
            output_dir,
            f"{safe_filename}_merged.json"
        )
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"  ê²°ê³¼ ì €ì¥: {output_file}")

    # ì „ì²´ ìš”ì•½ ì €ì¥
    summary_file = os.path.join(output_dir, "merge_summary.json")
    summary = {
        'total_files': len(json_files),
        'total_merged_groups': sum(len(r['merged_groups']) for r in all_results),
        'files': all_results
    }

    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print(f"\nì „ì²´ ìš”ì•½ ì €ì¥: {summary_file}")

    # PDF ì‹œê°í™” ìƒì„±
    print("\nPDF ì‹œê°í™” ìƒì„± ì¤‘...")
    pdf_path = create_visualization_pdf([str(f) for f in json_files], output_dir, font_name)
    print(f"PDF ìƒì„± ì™„ë£Œ: {pdf_path}")

    return all_results


if __name__ == "__main__":
    import sys
    import io

    # Windows ì½˜ì†” ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
    if sys.platform == 'win32':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

    input_dir = "table_output"
    output_dir = "merged_tables_output"

    print("=" * 60)
    print("í…Œì´ë¸” ë³‘í•© ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    print("=" * 60)

    results = process_json_files(input_dir, output_dir)

    print("\n" + "=" * 60)
    print("ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nê²°ê³¼ íŒŒì¼ ìœ„ì¹˜: {output_dir}/")
    print(f"- merge_summary.json: ì „ì²´ ìš”ì•½")
    print(f"- *_merged.json: ê° íŒŒì¼ë³„ ë³‘í•© ê²°ê³¼")
    print(f"- merged_tables_visualization.pdf: ì‹œê°í™” PDF")
