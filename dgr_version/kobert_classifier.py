"""
KoBERT ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸°
ì œëª© / ì„¤ëª… / ì œëª©ì•„ë‹˜ 3-class ë¶„ë¥˜
"""
import torch
import torch.nn as nn
from transformers import BertModel, AutoTokenizer
import numpy as np


class KoBERTClassifier(nn.Module):
    """KoBERT ê¸°ë°˜ 3-class ë¶„ë¥˜ ëª¨ë¸"""

    def __init__(self, num_classes=3, dropout_rate=0.1):
        super(KoBERTClassifier, self).__init__()

        # KoBERT ëª¨ë¸ ë¡œë“œ
        self.bert = BertModel.from_pretrained('skt/kobert-base-v1')

        # ë¶„ë¥˜ í—¤ë“œ
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(768, num_classes)  # KoBERT hidden size = 768

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        # BERT ì¸ì½”ë”© (token_type_ids ì œì™¸)
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # [CLS] í† í°ì˜ ì¶œë ¥ ì‚¬ìš©
        pooled_output = outputs.pooler_output

        # ë“œë¡­ì•„ì›ƒ + ë¶„ë¥˜
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        return logits


class TableTextClassifier:
    """í‘œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ wrapper í´ë˜ìŠ¤"""

    # í´ë˜ìŠ¤ ë ˆì´ë¸”
    LABELS = {
        0: "ì œëª©",
        1: "ì„¤ëª…",
        2: "ì œëª©ì•„ë‹˜"
    }

    def __init__(self, model_path=None, device=None):
        """
        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ ë¯¸í•™ìŠµ ëª¨ë¸)
            device: 'cuda' ë˜ëŠ” 'cpu'
        """
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)

        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained('skt/kobert-base-v1')

        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = KoBERTClassifier(num_classes=3)

        # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        if model_path:
            print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ: {model_path}")
            self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        else:
            print("âš ï¸  ë¯¸í•™ìŠµ ëª¨ë¸ (fine-tuning í•„ìš”)")

        self.model.to(self.device)
        self.model.eval()

    def predict(self, text, return_probs=False):
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¶„ë¥˜

        Args:
            text: ë¶„ë¥˜í•  í…ìŠ¤íŠ¸
            return_probs: Trueë©´ í™•ë¥ ê°’ë„ ë°˜í™˜

        Returns:
            ì˜ˆì¸¡ ë ˆì´ë¸” (str) ë˜ëŠ” (ë ˆì´ë¸”, í™•ë¥ ) íŠœí”Œ
        """
        # í† í¬ë‚˜ì´ì§•
        encoded = self.tokenizer(
            text,
            max_length=128,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        token_type_ids = encoded.get('token_type_ids')
        if token_type_ids is not None:
            token_type_ids = token_type_ids.to(self.device)

        # ì¶”ë¡ 
        with torch.no_grad():
            logits = self.model(input_ids, attention_mask, token_type_ids)
            probs = torch.softmax(logits, dim=-1)
            pred_class = torch.argmax(probs, dim=-1).item()
            pred_prob = probs[0, pred_class].item()

        label = self.LABELS[pred_class]

        if return_probs:
            return label, pred_prob, probs[0].cpu().numpy()
        else:
            return label

    def predict_batch(self, texts, batch_size=16):
        """
        ë°°ì¹˜ ì˜ˆì¸¡

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            [(í…ìŠ¤íŠ¸, ë ˆì´ë¸”, í™•ë¥ ), ...] ë¦¬ìŠ¤íŠ¸
        """
        results = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]

            # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
            encoded = self.tokenizer(
                batch_texts,
                max_length=128,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            input_ids = encoded['input_ids'].to(self.device)
            attention_mask = encoded['attention_mask'].to(self.device)
            token_type_ids = encoded.get('token_type_ids')
            if token_type_ids is not None:
                token_type_ids = token_type_ids.to(self.device)

            # ì¶”ë¡ 
            with torch.no_grad():
                logits = self.model(input_ids, attention_mask, token_type_ids)
                probs = torch.softmax(logits, dim=-1)
                pred_classes = torch.argmax(probs, dim=-1)

            # ê²°ê³¼ ì €ì¥
            for j, text in enumerate(batch_texts):
                pred_class = pred_classes[j].item()
                pred_prob = probs[j, pred_class].item()
                label = self.LABELS[pred_class]

                results.append((text, label, pred_prob))

        return results


# ========== í•™ìŠµìš© í•¨ìˆ˜ ==========
def train_kobert_classifier(train_data, val_data=None, epochs=3, batch_size=16, lr=2e-5, save_path='kobert_classifier.pt', pretrained_model_path=None):
    """
    KoBERT ë¶„ë¥˜ê¸° í•™ìŠµ

    Args:
        train_data: [(í…ìŠ¤íŠ¸, ë ˆì´ë¸”), ...] í˜•íƒœì˜ í•™ìŠµ ë°ì´í„°
                    ë ˆì´ë¸”: 0=ì œëª©, 1=ì„¤ëª…, 2=ì œëª©ì•„ë‹˜
        val_data: ê²€ì¦ ë°ì´í„° (ì„ íƒ)
        epochs: í•™ìŠµ ì—í­
        batch_size: ë°°ì¹˜ í¬ê¸°
        lr: í•™ìŠµë¥ 
        save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        pretrained_model_path: ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì¶”ê°€ í•™ìŠµìš©, Noneì´ë©´ ì²˜ìŒë¶€í„° í•™ìŠµ)
    """
    from torch.utils.data import Dataset, DataLoader
    from torch.optim import AdamW
    from tqdm import tqdm
    import os

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ í•™ìŠµ ë””ë°”ì´ìŠ¤: {device}")

    # ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    class TextDataset(Dataset):
        def __init__(self, data, tokenizer):
            self.data = data
            self.tokenizer = tokenizer

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            text, label = self.data[idx]

            encoded = self.tokenizer(
                text,
                max_length=128,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            # token_type_idsê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ìš´ í…ì„œ ìƒì„±
            token_type_ids = encoded.get('token_type_ids')
            if token_type_ids is None:
                token_type_ids = torch.zeros_like(encoded['input_ids'])

            return {
                'input_ids': encoded['input_ids'].squeeze(0),
                'attention_mask': encoded['attention_mask'].squeeze(0),
                'token_type_ids': token_type_ids.squeeze(0),
                'label': torch.tensor(label, dtype=torch.long)
            }

    # í† í¬ë‚˜ì´ì € & ëª¨ë¸
    tokenizer = AutoTokenizer.from_pretrained('skt/kobert-base-v1')
    model = KoBERTClassifier(num_classes=3)

    # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ìˆìœ¼ë©´)
    if pretrained_model_path and os.path.exists(pretrained_model_path):
        print(f"ğŸ“‚ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {pretrained_model_path}")
        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))
        print("âœ… ê¸°ì¡´ ëª¨ë¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤!")
    else:
        if pretrained_model_path:
            print(f"âš ï¸  ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pretrained_model_path}")
        print("ğŸ†• ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤")

    model.to(device)

    # ë°ì´í„°ë¡œë”
    train_dataset = TextDataset(train_data, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    if val_data:
        val_dataset = TextDataset(val_data, tokenizer)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)

    # ì˜µí‹°ë§ˆì´ì € & ì†ì‹¤í•¨ìˆ˜
    optimizer = AdamW(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # í•™ìŠµ ë£¨í”„
    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘ (ì—í­: {epochs}, ë°°ì¹˜: {batch_size}, í•™ìŠµë¥ : {lr})")

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        for batch in pbar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            token_type_ids = batch['token_type_ids'].to(device)
            labels = batch['label'].to(device)

            # Forward
            optimizer.zero_grad()
            logits = model(input_ids, attention_mask, token_type_ids)
            loss = criterion(logits, labels)

            # Backward
            loss.backward()
            optimizer.step()

            # í†µê³„
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=-1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})

        avg_loss = total_loss / len(train_loader)
        train_acc = 100 * correct / total

        print(f"Epoch {epoch+1} - Loss: {avg_loss:.4f}, Acc: {train_acc:.2f}%")

        # ê²€ì¦
        if val_data:
            model.eval()
            val_correct = 0
            val_total = 0

            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    token_type_ids = batch['token_type_ids'].to(device)
                    labels = batch['label'].to(device)

                    logits = model(input_ids, attention_mask, token_type_ids)
                    preds = torch.argmax(logits, dim=-1)

                    val_correct += (preds == labels).sum().item()
                    val_total += labels.size(0)

            val_acc = 100 * val_correct / val_total
            print(f"  ê²€ì¦ ì •í™•ë„: {val_acc:.2f}%")

    # ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), save_path)
    print(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")


# ========== ì‚¬ìš© ì˜ˆì‹œ ==========
if __name__ == '__main__':
    # 1. í•™ìŠµ ë°ì´í„° ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” ë¼ë²¨ë§ëœ ë°ì´í„° í•„ìš”)
    train_data = [
        # (í…ìŠ¤íŠ¸, ë ˆì´ë¸”: 0=ì œëª©, 1=ì„¤ëª…, 2=ì œëª©ì•„ë‹˜)
        ("í‘œ 3.2 ì—°ê°„ ì‹¤ì ", 0),
        ("â–¡ ì¶”ì§„ì¡°ì§ êµ¬ì„±", 0),
        ("í‘œ B.8 ì›”ë³„ ê¸°ì˜¨", 0),
        ("ã…‡ ì‚¬ì—… ê°œìš”", 0),
        ("â‘  ì£¼ìš” ë‚´ìš©", 0),

        ("ìƒì„¸í•œ ë‚´ìš©ì€ ë‹¤ìŒ í‘œ B.12ì™€ ê°™ë‹¤", 1),
        ("í‘œ A.20ì— ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤", 1),
        ("â€» ë³¸ ìë£ŒëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤", 1),

        ("123", 2),
        ("Â© 2024 All Rights Reserved", 2),
        ("ë‹¨ìœ„: â„ƒ", 2),
    ]

    # 2. ëª¨ë¸ í•™ìŠµ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
    # train_kobert_classifier(train_data, epochs=3, save_path='kobert_table_classifier.pt')

    # 3. ì¶”ë¡  ì˜ˆì‹œ
    print("\n========== ì¶”ë¡  ì˜ˆì‹œ ==========")
    classifier = TableTextClassifier(model_path=None)  # ë¯¸í•™ìŠµ ëª¨ë¸

    test_texts = [
        "í‘œ 4.21 ì˜ˆì‚° ì§‘í–‰ í˜„í™©",
        "ìƒì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ í‘œì™€ ê°™ë‹¤",
        "ë‹¨ìœ„: %"
    ]

    for text in test_texts:
        label, prob, all_probs = classifier.predict(text, return_probs=True)
        print(f"'{text}' â†’ {label} ({prob:.3f})")
        print(f"  ì „ì²´ í™•ë¥ : ì œëª©={all_probs[0]:.3f}, ì„¤ëª…={all_probs[1]:.3f}, ì œëª©ì•„ë‹˜={all_probs[2]:.3f}")
